## üöÄ Overview

This is a simple python script for the **Hacking LLMs Workshop**. It provides a simple command-line interface for interacting with a vulnerable LLM in a controlled remote environment.

## ‚öôÔ∏è Installation

You'll need `python` and `pip` installed on your device.

To install this CLI tool, run the following command in your terminal:

```bash
pip install -r requirements.txt
```

## üñ•Ô∏è Usage

To run the CLI tool, use the following command:

```bash
python main.py
```

#### üìö Additional Resources

- [OWASP Tallinn](https://owasp.ee/) - Learn more about our OWASP Chapter.
- [OWASP Main Page](https://owasp.org/)
- [OWASP GenAI Project](https://genai.owasp.org/) - A project focused on the security risks of AI systems.
- [Prompt Injection Risk](https://genai.owasp.org/llmrisk/llm01-prompt-injection/) - Understand the risks associated with prompt injection attacks.
- [Insecure Output Handling](https://genai.owasp.org/llmrisk/llm02-insecure-output-handling/) - Learn about the dangers of insecure output in language models.
  
- [Colab Notebook](https://colab.research.google.com/drive/1lIDc_R6VrksmfpT2DIBCilEwY-bTAD2q) - Try out our LLM security experiment on Data Poisoning in Google Colab.
